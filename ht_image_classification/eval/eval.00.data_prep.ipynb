{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation   \n",
    "The goal of this notebook is to take the inputs described below and generate a single CSV with only the information required to perform image-level processing and ad/cluster level aggregating, while performing basic sanity checks. \n",
    "\n",
    "### Inputs\n",
    "1. CP1_train_ads_labelled_fall2016.jsonl   \n",
    "This is a json lines file of ads that contain _id, class, cluster_id\n",
    "2. es_child_documents.jl   \n",
    "This is a json lines file of image objects which contain obj_stored_url and obj_parent\n",
    "3. image_url_to_valid_sha1.csv    \n",
    "This is a csv containing 2 columns: an image url (obj_stored_url), and a sha1 checksum of the file\n",
    "\n",
    "### Outputs\n",
    "1. CP1_data.csv    \n",
    "This is a csv file containing 4 columns: cluster_id, ad_id, image_sha, class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__depends__ = ['CP1_eval_ads_fall2016.jsonl',\n",
    "               'eval_es_child_documents.jl',\n",
    "               'eval_image_url_to_valid_sha1.csv']\n",
    "__dest__ = ['map.clusterId_to_adId.pickle',\n",
    "            'map.adId_to_clusterId.pickle',\n",
    "            'map.clusterId_to_class.pickle',\n",
    "            'map.class_to_clusterId.pickle',\n",
    "            'map.adId_to_sha1s.pickle',\n",
    "            'map.sha1_to_adIds.pickle',\n",
    "            'eval_data.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input file paths\n",
    "OFFICIAL_DATA_FILE = __depends__[0]\n",
    "ES_QUERY_RESULTS = __depends__[1]\n",
    "IMAGE_URL_SHA1_MAP = __depends__[2]\n",
    "# Mapping Cache Files\n",
    "CLUSTER_ID_TO_AD_ID = __dest__[0]\n",
    "AD_ID_TO_CLUSTER_ID = __dest__[1]\n",
    "CLUSTER_ID_TO_CLASS = __dest__[2]\n",
    "CLASS_TO_CLUSTER_ID = __dest__[3]\n",
    "AD_ID_TO_SHA1S = __dest__[4]\n",
    "SHA1_TO_AD_IDS = __dest__[5]\n",
    "# Full association mapping CSV file\n",
    "CP1_DATA_FILE = __dest__[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import cPickle as pickle\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checking the official data\n",
    "\n",
    "Assumptions:   \n",
    "1) The relationship between ad_id and cluster_id is many -> 1    \n",
    "2) The relationship between cluster_id and class is 1 -> 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbreviate the data to what we need: ad ids, cluster ids, and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_id_to_ad_ids = defaultdict(set)\n",
    "ad_id_to_cluster_ids = defaultdict(set)\n",
    "\n",
    "if os.path.isfile(CLUSTER_ID_TO_AD_ID) and os.path.isfile(AD_ID_TO_CLUSTER_ID):\n",
    "    print \"Loading existing intermediate results\"\n",
    "    with open(CLUSTER_ID_TO_AD_ID) as f:\n",
    "        cluster_id_to_ad_ids = pickle.load(f)\n",
    "    with open(AD_ID_TO_CLUSTER_ID) as f:\n",
    "        ad_id_to_cluster_ids = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    print \"Extracting relevant info from JSONL\"\n",
    "    with open(OFFICIAL_DATA_FILE) as infile:\n",
    "        for line in infile:\n",
    "            document = json.loads(line.strip())\n",
    "\n",
    "            cluster_id_to_ad_ids[document['cluster_id']].add(document['_id'])\n",
    "            ad_id_to_cluster_ids[document['_id']].add(document['cluster_id'])\n",
    "           \n",
    "    print \"Saving intermediate results\"\n",
    "    with open(CLUSTER_ID_TO_AD_ID, 'wb') as f:\n",
    "        pickle.dump(cluster_id_to_ad_ids, f, -1)\n",
    "    with open(AD_ID_TO_CLUSTER_ID, 'wb') as f:\n",
    "        pickle.dump(ad_id_to_cluster_ids, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check that each cluster has at least one ad\n",
    "for cluster_id, ad_ids in cluster_id_to_ad_ids.iteritems():\n",
    "    assert len(ad_ids) > 0\n",
    "    \n",
    "# Sanity check no ad falls in more than one cluster (assumption 1)\n",
    "all_ad_ids = []\n",
    "num_unique_ad_ids = 0\n",
    "\n",
    "for _, ad_ids in cluster_id_to_ad_ids.iteritems():\n",
    "    all_ad_ids += list(ad_ids)\n",
    "    num_unique_ad_ids += len(ad_ids)\n",
    "    \n",
    "assert len(all_ad_ids) == num_unique_ad_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Official data descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ads_per_positive_cluster = [len(ad_ids) for cid, ad_ids in cluster_id_to_ad_ids.iteritems()]\n",
    "print 'min/med/avg/max/total ads per positive cluster: %d/%d/%d/%d/%d' % (min(ads_per_positive_cluster),\n",
    "                                                                          np.median(ads_per_positive_cluster),\n",
    "                                                                          np.average(ads_per_positive_cluster),\n",
    "                                                                          max(ads_per_positive_cluster),\n",
    "                                                                          sum(ads_per_positive_cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associating Imagery\n",
    "\n",
    "The shas present here have already been vetted by SMQTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(AD_ID_TO_SHA1S) and os.path.isfile(SHA1_TO_AD_IDS):\n",
    "    print \"Loading cached maps\"\n",
    "    ad_id_to_shas = pickle.load(open(AD_ID_TO_SHA1S))\n",
    "    sha_to_ad_ids = pickle.load(open(SHA1_TO_AD_IDS))\n",
    "else:\n",
    "    ad_id_to_shas = defaultdict(set)\n",
    "    sha_to_ad_ids = defaultdict(set)\n",
    "    ad_id_to_image_urls = defaultdict(set)\n",
    "    image_url_to_ad_ids = defaultdict(set)\n",
    "\n",
    "    print 'Read adId->imageSha relationships from ElasticSearch query results'\n",
    "    with open(ES_QUERY_RESULTS) as infile:\n",
    "        for line in infile:\n",
    "            document = json.loads(line.strip())\n",
    "\n",
    "            if isinstance(document['obj_parent'], list):\n",
    "                ad_ids = document['obj_parent']\n",
    "            else:\n",
    "                ad_ids = [document['obj_parent']]\n",
    "\n",
    "            for ad_id in ad_ids:\n",
    "                if document['obj_stored_url']:\n",
    "                    ad_id_to_image_urls[ad_id].add(document['obj_stored_url'])\n",
    "                    image_url_to_ad_ids[document['obj_stored_url']].add(ad_id)\n",
    "\n",
    "    print 'Read in [URL,SHA1] pairs for computable images'\n",
    "    image_url_to_sha = {}\n",
    "    image_sha_to_url = {}\n",
    "    with open(IMAGE_URL_SHA1_MAP) as infile:\n",
    "        for (image_url, sha1) in csv.reader(infile):\n",
    "            image_url_to_sha[image_url] = sha1\n",
    "            image_sha_to_url[sha1] = image_url\n",
    "\n",
    "    print 'Construct map of ad ID to image SHA1s in that ad'\n",
    "    for (ad_id, image_urls) in ad_id_to_image_urls.iteritems():\n",
    "        try:\n",
    "            ad_image_shas = set([image_url_to_sha[url] for url in image_urls])\n",
    "            ad_id_to_shas[ad_id] = ad_image_shas\n",
    "            for sha in ad_image_shas:\n",
    "                sha_to_ad_ids[sha].add(ad_id)\n",
    "        except KeyError:\n",
    "            # There might not be a sha1 for the image url since some shas were invalid (from SMQTK) \n",
    "            pass\n",
    "    \n",
    "    print 'Save mappings'\n",
    "    with open(AD_ID_TO_SHA1S, 'wb') as f:\n",
    "        pickle.dump(ad_id_to_shas, f, -1)\n",
    "    with open(SHA1_TO_AD_IDS, 'wb') as f:\n",
    "        pickle.dump(sha_to_ad_ids, f, -1)\n",
    "        \n",
    "    del ad_id_to_image_urls, image_url_to_ad_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sanity check that each ad has at least 1 sha\n",
    "for shas in ad_id_to_shas.values():\n",
    "    assert len(shas) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shas_per_ad = map(len, ad_id_to_shas.values())\n",
    "print 'min/med/avg/max/total images per ad: %d/%d/%d/%d/%d' % (min(shas_per_ad),\n",
    "                                                               np.median(shas_per_ad),\n",
    "                                                               np.average(shas_per_ad),\n",
    "                                                               max(shas_per_ad),\n",
    "                                                               sum(shas_per_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ad_per_sha = map(len, ad_id_to_shas.values())\n",
    "print 'min/med/avg/max/total ads per SHA1: %d/%d/%d/%d/%d' % (min(ad_per_sha),\n",
    "                                                               np.median(ad_per_sha),\n",
    "                                                               np.average(ad_per_sha),\n",
    "                                                               max(ad_per_sha),\n",
    "                                                               sum(ad_per_sha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check: All ads have at least one image\n",
    "c = 0\n",
    "for ad, shas in ad_id_to_shas.iteritems():\n",
    "    if len(shas) == 0:\n",
    "        assert \"Ad with no images:\", ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one CSV with all the relevant information, in the format of:    \n",
    "cluster_id, ad_id, image_sha, class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(CP1_DATA_FILE, 'w') as outfile:\n",
    "    writer = csv.writer(outfile, lineterminator='\\n')\n",
    "    \n",
    "    for (cluster_id, ad_ids) in cluster_id_to_ad_ids.iteritems():\n",
    "        for ad_id in ad_ids:\n",
    "            for image_sha in ad_id_to_shas[ad_id]:\n",
    "                writer.writerow([cluster_id, ad_id, image_sha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print clusters ordered by number of images\n",
    "clusters = defaultdict(set)\n",
    "\n",
    "with open(CP1_DATA_FILE) as infile:\n",
    "    for (cid, ad_id, sha) in csv.reader(infile):\n",
    "        clusters[cid].add(sha)\n",
    "\n",
    "clusters_by_size = sorted(clusters.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "for (cluster_id, shas) in clusters_by_size:\n",
    "    print '%s %d' % (cluster_id, len(shas))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
