{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation   \n",
    "The goal of this notebook is to take the inputs described below and generate a single CSV with only the information required to perform image-level processing and ad/cluster level aggregating, while performing basic sanity checks. \n",
    "\n",
    "### Inputs\n",
    "1. CP1_train_ads_labelled_fall2016.jsonl   \n",
    "This is a json lines file of ads that contain _id, class, cluster_id\n",
    "2. es_child_documents.jl   \n",
    "This is a json lines file of image objects which contain obj_stored_url and obj_parent\n",
    "3. image_url_to_valid_sha1.csv    \n",
    "This is a csv containing 2 columns: an image url (obj_stored_url), and a sha1 checksum of the file\n",
    "\n",
    "### Outputs\n",
    "1. CP1_data.csv    \n",
    "This is a csv file containing 4 columns: cluster_id, ad_id, image_sha, class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__depends__ = ['CP1_train_ads_labelled_fall2016.jsonl',\n",
    "               'es_child_documents.jl',\n",
    "               'image_url_to_valid_sha1.csv']\n",
    "__dest__ = ['CP1_data.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OFFICIAL_DATA_FILE = 'CP1_train_ads_labelled_fall2016.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checking the official data\n",
    "\n",
    "Assumptions:   \n",
    "1) The relationship between ad_id and cluster_id is many -> 1    \n",
    "2) The relationship between cluster_id and class is 1 -> 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbreviate the data to what we need: ad ids, cluster ids, and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_id_to_ad_ids = defaultdict(set)\n",
    "cluster_id_to_class = defaultdict(set)\n",
    "\n",
    "with open(OFFICIAL_DATA_FILE) as infile:\n",
    "    for line in infile:\n",
    "        document = json.loads(line.strip())\n",
    "        \n",
    "        cluster_id_to_ad_ids[document['cluster_id']].add(document['_id'])\n",
    "        cluster_id_to_class[document['cluster_id']].add(document['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check that each cluster has at least one ad\n",
    "for cluster_id, ad_ids in cluster_id_to_ad_ids.iteritems():\n",
    "    assert len(ad_ids) > 0\n",
    "    \n",
    "# Sanity check no ad falls in more than one cluster (assumption 1)\n",
    "all_ad_ids = []\n",
    "num_unique_ad_ids = 0\n",
    "\n",
    "for _, ad_ids in cluster_id_to_ad_ids.iteritems():\n",
    "    all_ad_ids += list(ad_ids)\n",
    "    num_unique_ad_ids += len(ad_ids)\n",
    "    \n",
    "assert len(all_ad_ids) == num_unique_ad_ids\n",
    "\n",
    "# Sanity check that each cluster only belongs to one class (assumption 2) \n",
    "for _, cls in cluster_id_to_class.iteritems():\n",
    "    assert len(cls) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Official data descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print '%d clusters (%d positive, %d negative)' % (len(cluster_id_to_class),\n",
    "                                                  len([x for x in cluster_id_to_class.values() if x == {1}]),\n",
    "                                                  len([x for x in cluster_id_to_class.values() if x == {0}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ads_per_positive_cluster = [len(ad_ids) for cid, ad_ids in cluster_id_to_ad_ids.iteritems() \\\n",
    "                            if cluster_id_to_class[cid] == {1}]\n",
    "print 'min/med/avg/max/total ads per positive cluster: %d/%d/%d/%d/%d' % (min(ads_per_positive_cluster),\n",
    "                                                                          np.median(ads_per_positive_cluster),\n",
    "                                                                          np.average(ads_per_positive_cluster),\n",
    "                                                                          max(ads_per_positive_cluster),\n",
    "                                                                          sum(ads_per_positive_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ads_per_negative_cluster = [len(ad_ids) for cid, ad_ids in cluster_id_to_ad_ids.iteritems() \\\n",
    "                            if cluster_id_to_class[cid] == {0}]\n",
    "print 'min/med/avg/max/total ads per negative cluster: %d/%d/%d/%d/%d' % (min(ads_per_negative_cluster),\n",
    "                                                                          np.median(ads_per_negative_cluster),\n",
    "                                                                          np.average(ads_per_negative_cluster),\n",
    "                                                                          max(ads_per_negative_cluster),\n",
    "                                                                          sum(ads_per_negative_cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associating Imagery\n",
    "\n",
    "The shas present here have already been vetted by SMQTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ad_id_to_shas = defaultdict(set)\n",
    "ad_id_to_image_urls = defaultdict(set)\n",
    "\n",
    "\n",
    "with open('es_child_documents.jl') as infile:\n",
    "    for line in infile:\n",
    "        document = json.loads(line.strip())\n",
    "        \n",
    "        if isinstance(document['obj_parent'], list):\n",
    "            ad_ids = document['obj_parent']\n",
    "        else:\n",
    "            ad_ids = [document['obj_parent']]\n",
    "            \n",
    "        for ad_id in ad_ids:\n",
    "            if document['obj_stored_url']:\n",
    "                ad_id_to_image_urls[ad_id].add(document['obj_stored_url'])\n",
    "        \n",
    "image_url_to_sha = {}\n",
    "with open('image_url_to_valid_sha1.csv') as infile:\n",
    "    for (image_url, sha1) in csv.reader(infile):\n",
    "        image_url_to_sha[image_url] = sha1\n",
    "        \n",
    "\n",
    "for (ad_id, image_urls) in ad_id_to_image_urls.iteritems():\n",
    "    try:\n",
    "        ad_id_to_shas[ad_id] = set([image_url_to_sha[url] for url in image_urls])\n",
    "    except KeyError:\n",
    "        # There might not be a sha1 for the image url since some shas were invalid (from SMQTK) \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity check that each ad has at least 1 sha\n",
    "for shas in ad_id_to_shas.values():\n",
    "    assert len(shas) > 0\n",
    "    \n",
    "# Sanity check that each cluster has at least 1 ad with at least 1 sha\n",
    "for (cluster_id, ad_ids) in cluster_id_to_ad_ids.iteritems():\n",
    "    cluster_shas = set()\n",
    "    for ad_id in ad_ids:\n",
    "        cluster_shas |= ad_id_to_shas[ad_id]\n",
    "        \n",
    "    if not len(cluster_shas) > 0:\n",
    "        print cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shas_per_ad = map(len, ad_id_to_shas.values())\n",
    "print 'min/med/avg/max/total images per ad: %d/%d/%d/%d/%d' % (min(shas_per_ad),\n",
    "                                                               np.median(shas_per_ad),\n",
    "                                                               np.average(shas_per_ad),\n",
    "                                                               max(shas_per_ad),\n",
    "                                                               sum(shas_per_ad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one CSV with all the relevant information, in the format of:    \n",
    "cluster_id, ad_id, image_sha, class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('CP1_data.csv', 'w') as outfile:\n",
    "    writer = csv.writer(outfile, lineterminator='\\n')\n",
    "    \n",
    "    for (cluster_id, ad_ids) in cluster_id_to_ad_ids.iteritems():\n",
    "        for ad_id in ad_ids:\n",
    "            for image_sha in ad_id_to_shas[ad_id]:\n",
    "                writer.writerow([cluster_id, ad_id, image_sha, list(cluster_id_to_class[cluster_id])[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find shas that are marked positive and negative\n",
    "sha_to_class = defaultdict(set)\n",
    "\n",
    "with open('CP1_data.csv') as infile:\n",
    "    for (cid, ad_id, sha, cls) in csv.reader(infile):\n",
    "        sha_to_class[sha].add(cls)\n",
    "        \n",
    "bad_shas = set([sha for sha, classes in sha_to_class.iteritems() if len(classes) > 1])\n",
    "\n",
    "print len(bad_shas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print clusters ordered by number of images\n",
    "clusters = defaultdict(set)\n",
    "\n",
    "with open('CP1_data.csv') as infile:\n",
    "    for (cid, ad_id, sha, cls) in csv.reader(infile):\n",
    "        clusters[cid].add(sha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters_by_size = sorted(clusters.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "for (cluster_id, shas) in clusters_by_size:\n",
    "    print '%s %s %d' % (cluster_id, cluster_id_to_class[cluster_id], len(shas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
